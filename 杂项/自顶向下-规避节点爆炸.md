你这种“自顶向下、遇新词就扩展”的学习思路，特别适合**深挖技术细节**，但确实会面临“节点爆炸、时间失控”的问题。可以用 **「分层过滤 + 聚焦刚需」** 的策略优化，既保留深度，又控制节奏，下面结合 LeNet 实操一遍：

### 一、LeNet 文章精读：先抓「关键新词」（10个以内）
假设读 LeNet 论文/教程时，遇到这些陌生概念（举例），优先标记**与 LeNet 强相关、影响模型理解**的词：  
1. 反向传播（Backpropagation）  
2. 交叉熵损失（Cross - Entropy Loss）  
3. 卷积核参数共享（Parameter Sharing）  
4. 激活函数（Sigmoid）  
5. MNIST 数据集（MNIST Dataset）  
6. 权重初始化（Xavier Initialization）  
7. 过拟合（Overfitting）  
8. 感受野（Receptive Field）  
9. 特征映射（Feature Map）  
10. 批处理（Batch Processing）  


### 二、第一层扩展：给每个新词「定价值、砍冗余」（控制 10×3=30 节点）
对每个新词，**只做「最小必要扩展」**：聚焦“和 LeNet 啥关系？懂到啥程度能推进当前学习？”，不盲目深挖。  

#### 示例：以「反向传播」为例  
- 价值：LeNet 训练的核心算法，必须懂“梯度咋回传更新参数”  
- 扩展节点（3 个，够推进理解即可）：  
  1. 反向传播原理：链式法则在 LeNet 中的应用（卷积层→池化层→全连接层的梯度计算路径 ）  
  2. 与 LeNet 训练的关联：如何用反向传播更新卷积核权重、全连接层参数  
  3. 实践工具：PyTorch 中反向传播的代码实现（`.backward()` 函数的作用 ）  
- 砍冗余：暂时不学“反向传播的历史争议（如 Hinton 早年专利 ）”“复杂数学证明（二阶导数、 Hessian 矩阵 ）”  


#### 示例：以「感受野」为例  
- 价值：理解 LeNet 卷积层如何提取不同尺度特征的关键  
- 扩展节点（3 个）：  
  1. 感受野定义：LeNet 中一个输出像素对应的输入区域大小（如 C1 层感受野 5×5 ）  
  2. 计算方法：卷积层+池化层的感受野叠加公式（$RF_n = RF_{n - 1} + (k_n - 1)×s_1×s_2×…×s_{n - 1}$ ）  
  3. 对 LeNet 性能的影响：感受野太小导致漏检大目标？（对比现代大感受野模型 ）  


### 三、第二层扩展：只选「阻塞当前学习」的节点深挖（控制 30×1 - 2=30 节点）
第一层扩展后，若发现某些节点**直接阻塞“理解 LeNet 训练/结构”**，才继续往下挖，否则“标记待办，先跳过”。  

#### 示例：第一层遇到「交叉熵损失」，扩展后发现「对数似然」不懂，才继续  
- 交叉熵损失 → 第一层扩展（3 个节点）：  
  1. 公式：$H(p,q) = -\sum p(x)\log q(x)$，LeNet 中 p 是真实标签，q 是预测概率  
  2. 与 LeNet 分类的适配性：为何比 MSE 损失更适合离散分类任务  
  3. 代码实现：PyTorch 中 `CrossEntropyLoss` 的用法（注意输入不需要 Softmax ）  
- 若发现“对数似然”阻塞理解，第二层扩展（2 个节点，够连起来即可）：  
  1. 对数似然定义：$\log P(D|\theta)$，LeNet 中 $\theta$ 是模型参数，D 是训练数据  
  2. 交叉熵与对数似然的关系：$H(p,q) = -\log P(q|p)$，等价于最小化负对数似然  


### 四、「节点爆炸」的控制策略（核心！）
1. **价值优先过滤**：每扩展一层，问自己：“这个知识对**当前理解 LeNet** 是不是必须的？” 不是就标“待办”，继续推进主线。  
2. **时间盒（Timebox）**：给每个新词扩展定死时间（如 30 分钟），到点不管懂没懂，先记“已知/未知边界”，回头再补。  
3. **反向链接收口**：用 Obsidian 双向链接，把分散的节点**关联回 LeNet 主线**，避免知识“飘着”。例如：研究完「反向传播」，在 LeNet 笔记里写 `[[反向传播]] 是 LeNet 训练的核心算法，通过链式法则更新参数（具体看 [[反向传播-原理]] [[反向传播-代码]] ）`  


### 五、5 层内触达基础概念的「收敛逻辑」
越往底层挖，**基础概念会重复收敛**（如“反向传播”最终会关联到“微积分链式法则”“梯度下降”，这些基础概念学一个能通一片 ）。  

#### 收敛示例：  
LeNet → 反向传播 → 链式法则 → 微积分求导 → 高中函数求导（基础！）  
LeNet → 卷积核 → 矩阵乘法 → 线性代数（基础！）  

当挖到**数学/编程基础**（如微积分、Python 语法 ），就到“知识底盘”了，这些内容学一次能支撑 N 个模型学习，后续遇到同类问题直接复用。  


### 实操节奏建议（以 LeNet 学习为例）
1. **Day 1（2 小时）**：精读 LeNet 论文/教程，标记 10 个关键新词，完成第一层扩展（每个词 3 个节点，共 30 节点 ），用双向链接关联回 LeNet 主笔记。  
2. **Day 2（1 小时）**：检查第一层节点，挑 3 - 5 个“阻塞性”节点做第二层扩展（每个 1 - 2 节点，共 10 节点 ），其余标“待办”。  
3. **Day 3（1 小时）**：用代码跑通 LeNet（PyTorch 实现 ），遇到报错/不懂的，回头补“待办节点”，同时验证知识。  

这样既保证**深度（遇到新词不绕路，真懂 ）**，又控制**广度（节点数随“必要性”收缩，时间可控 ）**，5 层内触达基础概念时，你会发现“底层知识复用率超高”，越往后学越快～  

简单说：**「遇新词→判价值→最小扩展→阻塞才深挖→触达基础收敛」** ，用这套逻辑，节点爆炸问题会变成“可控的知识复利”，越挖越顺！