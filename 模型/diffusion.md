---
tags:
  - 模型类型/生成模型
  - "#任务/生成"
  - "#阶段/前沿"
  - "#知识域/生成对抗"
---
- Diffusion Model
- 前沿价值：近年生成模型新宠，在图像生成（如 Stable Diffusion ）、文生图（AIGC 核心技术 ）领域爆火，通过 “逐步去噪” 生成高质量数据，原理与 GAN、VAE（变分自编码器 ）不同。
- 学习突破：
- 原理啃透：从论文《Denoising Diffusion Probabilistic Models》开始，理解 “前向扩散（给真实数据逐步加噪声，变成随机噪声 ）、反向扩散（从噪声逐步去噪，生成新数据 ）” 的过程，掌握变分推断、概率建模的数学逻辑（偏理论，但需理解核心流程 ）。对比 Diffusion 与 GAN、VAE 的生成差异（Diffusion 生成更稳定、细节丰富，但训练推断耗时 ）。
- 实践尝试：用 PyTorch 实现简易 Diffusion 模型，在 MNIST、CIFAR - 10 数据集生成图像，感受去噪过程。研究 Stable Diffusion 等开源项目，理解如何结合文本编码器（如 CLIP ）实现文生图（文本特征引导扩散去噪方向 ）。尝试优化采样步骤（如加快去噪速度，提升生成效率 ），或微调模型适配特定风格生成（如动漫、油画 ）。
- 生态追踪：关注 Diffusion 在 3D 生成（如 3D 物体、场景 ）、视频生成（逐帧扩散 ）的拓展，了解模型蒸馏（让大模型在小设备跑 ）、可控生成（如控制人物姿势、表情 ）等前沿方向，把握 AIGC 技术演进趋势。